#!/usr/bin/env python3
import os
import json
import logging
import time
from collections import defaultdict, deque

import numpy as np
import lightgbm as lgb
from kafka import KafkaConsumer, KafkaProducer
from kafka.errors import NoBrokersAvailable

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger('ml-consumer')

# ---------- –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è ----------
BOOTSTRAP_SERVERS = os.getenv('KAFKA_BOOTSTRAP_SERVERS', 'kafka-0:9092,kafka-1:9092').split(',')
INPUT_TOPIC = os.getenv('INPUT_TOPIC', 'ml-input')
OUTPUT_TOPIC = os.getenv('OUTPUT_TOPIC', 'ml-result')
GROUP_ID = os.getenv('GROUP_ID', 'ml-group')

WINDOW_SIZE = int(os.getenv('WINDOW_SIZE', '10'))               # —Ä–∞–∑–º–µ—Ä –æ–∫–Ω–∞ –¥–ª—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–ª–∞–≥–∏)
MIN_TRAIN_SAMPLES = int(os.getenv('MIN_TRAIN_SAMPLES', '100'))  # –º–∏–Ω. —Ç–æ—á–µ–∫ –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
MAX_HISTORY = int(os.getenv('MAX_HISTORY', '500'))              # –º–∞–∫—Å. —Ö—Ä–∞–Ω–∏–º—ã—Ö —Ç–æ—á–µ–∫ –Ω–∞ —Ç–∏–∫–µ—Ä
PREDICTION_STEPS = int(os.getenv('PREDICTION_STEPS', '3'))      # —Å–∫–æ–ª—å–∫–æ —à–∞–≥–æ–≤ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å
RETRAIN_EVERY = int(os.getenv('RETRAIN_EVERY', '50'))           # –ø–µ—Ä–µ–æ–±—É—á–∞—Ç—å –∫–∞–∂–¥—ã–µ N –Ω–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π

# –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã LightGBM (–º–æ–∂–Ω–æ –º–µ–Ω—è—Ç—å —á–µ—Ä–µ–∑ –æ–∫—Ä—É–∂–µ–Ω–∏–µ)
LGB_PARAMS = {
    'n_estimators': int(os.getenv('LGB_N_ESTIMATORS', '100')),
    'max_depth': int(os.getenv('LGB_MAX_DEPTH', '5')),
    'learning_rate': float(os.getenv('LGB_LEARNING_RATE', '0.1')),
    'subsample': float(os.getenv('LGB_SUBSAMPLE', '0.8')),
    'colsample_bytree': float(os.getenv('LGB_COLSAMPLE_BYTREE', '0.8')),
    'num_leaves': int(os.getenv('LGB_NUM_LEAVES', '31')),
    'random_state': 42,
    'n_jobs': 1,
    'verbose': -1  # –æ—Ç–∫–ª—é—á–∞–µ–º –ª–∏—à–Ω–∏–µ –ª–æ–≥–∏ LightGBM
}

# ---------- –ì–ª–æ–±–∞–ª—å–Ω—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö ----------
price_history = defaultdict(lambda: deque(maxlen=MAX_HISTORY))   # —Å—ã—Ä—ã–µ —Ü–µ–Ω—ã
models = {}                                                      # –æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –ø–æ —Ç–∏–∫–µ—Ä–∞–º
is_trained = defaultdict(bool)                                   # —Ñ–ª–∞–≥ –ø–µ—Ä–≤–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
msg_count_since_train = defaultdict(int)                         # —Å—á—ë—Ç—á–∏–∫ —Å–æ–æ–±—â–µ–Ω–∏–π –ø–æ—Å–ª–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è

# ---------- –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ----------
def create_consumer():
    for attempt in range(10):
        try:
            consumer = KafkaConsumer(
                INPUT_TOPIC,
                bootstrap_servers=BOOTSTRAP_SERVERS,
                group_id=GROUP_ID,
                auto_offset_reset='latest',
                enable_auto_commit=True,
                value_deserializer=lambda m: json.loads(m.decode('utf-8'))
            )
            logger.info("Kafka consumer created")
            return consumer
        except NoBrokersAvailable:
            logger.warning(f"No brokers available, retrying {attempt+1}/10...")
            time.sleep(5)
    raise Exception("Could not create consumer after retries")

def create_producer():
    for attempt in range(10):
        try:
            producer = KafkaProducer(
                bootstrap_servers=BOOTSTRAP_SERVERS,
                value_serializer=lambda v: json.dumps(v).encode('utf-8')
            )
            logger.info("Kafka producer created")
            return producer
        except NoBrokersAvailable:
            logger.warning(f"No brokers available for producer, retrying {attempt+1}/10...")
            time.sleep(5)
    raise Exception("Could not create producer after retries")

def extract_features(prices_window):
    """
    –ò–∑ –º–∞—Å—Å–∏–≤–∞ —Ü–µ–Ω (–¥–ª–∏–Ω–æ–π WINDOW_SIZE) —Å—Ç—Ä–æ–∏—Ç –≤–µ–∫—Ç–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–¥–Ω–æ–º–µ—Ä–Ω—ã–π numpy –º–∞—Å—Å–∏–≤.
    """
    prices = np.array(prices_window)
    features = []
    # 1. –°–∞–º–∏ —Ü–µ–Ω—ã (–ª–∞–≥–∏)
    features.extend(prices)
    # 2. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    features.append(np.mean(prices))
    features.append(np.std(prices))
    features.append(np.min(prices))
    features.append(np.max(prices))
    # 3. –î–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏ (returns)
    if len(prices) > 1:
        returns = np.diff(prices) / prices[:-1]
        features.append(np.mean(returns))
        features.append(np.std(returns))
    else:
        features.extend([0.0, 0.0])
    return np.array(features)

def prepare_training_data(ticker):
    """
    –ü–æ –∏—Å—Ç–æ—Ä–∏–∏ —Ü–µ–Ω –¥–ª—è —Ç–∏–∫–µ—Ä–∞ —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç –º–∞—Ç—Ä–∏—Ü—É X (–ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –æ–∫–Ω–∞)
    –∏ –≤–µ–∫—Ç–æ—Ä y (—Ü–µ–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è ‚Äì —Å–ª–µ–¥—É—é—â–∞—è —Ü–µ–Ω–∞).
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (X, y) –∏–ª–∏ (None, None), –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ.
    """
    history = list(price_history[ticker])
    if len(history) < WINDOW_SIZE + 1:
        return None, None
    X, y = [], []
    for i in range(WINDOW_SIZE, len(history)):
        window = history[i-WINDOW_SIZE:i]
        features = extract_features(window)
        X.append(features)
        y.append(history[i])
    return np.array(X), np.array(y)

def train_model(ticker):
    """–û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å LightGBM –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ —Ç–∏–∫–µ—Ä–∞ –Ω–∞ –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö."""
    X, y = prepare_training_data(ticker)
    if X is None or len(X) == 0:
        logger.warning(f"Not enough data to train model for {ticker}")
        return None
    model = lgb.LGBMRegressor(**LGB_PARAMS)
    model.fit(X, y)
    logger.info(f"Trained LightGBM for {ticker} on {len(X)} samples")
    return model

def predict_multi_step(ticker, steps=PREDICTION_STEPS):
    """–ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —Å–ª–µ–¥—É—é—â–∏–µ steps –∑–Ω–∞—á–µ–Ω–∏–π —Ü–µ–Ω—ã –¥–ª—è —Ç–∏–∫–µ—Ä–∞ —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ."""
    if ticker not in models:
        return None
    history = list(price_history[ticker])
    if len(history) < WINDOW_SIZE:
        return None
    # –ë–µ—Ä—ë–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ –æ–∫–Ω–æ
    current_window = history[-WINDOW_SIZE:]
    predictions = []
    for _ in range(steps):
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ —Ç–µ–∫—É—â–µ–≥–æ –æ–∫–Ω–∞
        features = extract_features(current_window).reshape(1, -1)
        pred = models[ticker].predict(features)[0]
        predictions.append(pred)
        # –û–±–Ω–æ–≤–ª—è–µ–º –æ–∫–Ω–æ: —Å–¥–≤–∏–≥–∞–µ–º –∏ –¥–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
        current_window.pop(0)
        current_window.append(pred)
    return predictions

# ---------- –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª ----------
def main():
    logger.info(f"Starting ML consumer with LightGBM, steps={PREDICTION_STEPS}, retrain_every={RETRAIN_EVERY}")
    consumer = create_consumer()
    producer = create_producer()

    try:
        for msg in consumer:
            data = msg.value
            ticker = data.get('ticker')
            close_price = data.get('close_price')
            if not ticker or close_price is None:
                logger.debug(f"Skipping message without ticker/price: {data}")
                continue
            try:
                close_price = float(close_price)
            except (TypeError, ValueError):
                logger.warning(f"Invalid close_price: {close_price}")
                continue

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ü–µ–Ω—É –≤ –∏—Å—Ç–æ—Ä–∏—é
            price_history[ticker].append(close_price)
            logger.debug(f"Received {ticker}: {close_price}")

            # --- –û–±—É—á–µ–Ω–∏–µ / –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ ---
            if not is_trained[ticker]:
                # –ü–µ—Ä–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ
                if len(price_history[ticker]) >= MIN_TRAIN_SAMPLES:
                    model = train_model(ticker)
                    if model:
                        models[ticker] = model
                        is_trained[ticker] = True
                        msg_count_since_train[ticker] = 0
                        logger.info(f"First model trained for {ticker}")
            else:
                # –ú–æ–¥–µ–ª—å —É–∂–µ –µ—Å—Ç—å ‚Äì —Å—á–∏—Ç–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –∏ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –ø–µ—Ä–µ–æ–±—É—á–∞–µ–º
                msg_count_since_train[ticker] += 1
                if msg_count_since_train[ticker] >= RETRAIN_EVERY:
                    logger.info(f"Retraining model for {ticker} after {RETRAIN_EVERY} messages")
                    new_model = train_model(ticker)
                    if new_model:
                        models[ticker] = new_model
                        msg_count_since_train[ticker] = 0
                        logger.info(f"Model retrained for {ticker}")
                    else:
                        # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ–æ–±—É—á–∏—Ç—å (–Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö), —Å–±—Ä–∞—Å—ã–≤–∞—Ç—å —Å—á—ë—Ç—á–∏–∫ –Ω–µ –±—É–¥–µ–º
                        logger.warning(f"Retraining failed for {ticker}, will retry later")

            # --- –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ (–µ—Å–ª–∏ –º–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞) ---
            if is_trained[ticker]:
                predictions = predict_multi_step(ticker)
                if predictions:
                    logger.info(f"üîÆ Predictions for {ticker}: {[round(p,2) for p in predictions]}")
                    result_message = {
                        'ticker': ticker,
                        'predictions': [round(p, 2) for p in predictions],
                        'steps': PREDICTION_STEPS,
                        'timestamp': time.time()
                    }
                    producer.send(OUTPUT_TOPIC, value=result_message)

    except KeyboardInterrupt:
        logger.info("Shutting down...")
    finally:
        consumer.close()
        producer.flush()
        producer.close()

if __name__ == '__main__':
    main()